---
output:
  word_document: default
  html_document: default
---


```{r}
library(tidyverse)
library(tidymodels)
library(lubridate)

bike <- read_csv("bike_cleaned.csv")
bike <- bike %>% mutate(dteday = mdy(dteday))
bike <- bike %>% mutate(season = as_factor(season)) %>%
  mutate(holiday = as_factor(holiday)) %>%
  mutate(mnth = as_factor(mnth)) %>%
  mutate(weekday = as_factor(weekday)) %>%
  mutate(workingday = as_factor(workingday)) %>%
  mutate(weathersit = as_factor(weathersit)) %>%
  mutate(hr = as_factor(hr))
```


```{r Task 1 spliting data into training and testing w/ 70% in training}
set.seed(1234)
bike_split <- initial_split(bike, prop =0.70, strata = count)
train <- training(bike_split)
test <- testing(bike_split)


```

```{r Task 2}
str(test)
str(train)
```
 
 
 Task 2
 test = 4347 rows of data
 training = 13032 rows of data
 
```{r Task 3}
count_recipe <- recipe(count ~ mnth + season + hr + holiday + weekday + temp + weathersit, bike) %>%
  step_dummy(mnth)

lm_model <-
  linear_reg() %>%
  set_engine("lm")

lm_wflow <-
  workflow() %>%
  add_model(lm_model) %>%
  add_recipe(count_recipe)

lm_fit <- fit(lm_wflow, bike)
summary(lm_fit$fit$fit$fit)

```
 
Task 3
The model produced an adjusted R square of 0.6232. Collectively the model highlights many significant variables (hr, season, etc.).  However, our work with other models produced a better result(Ridge and Lasso).

```{r Task 4 predict function}

# predict_train <- lm_fit %>% predict(test) %>% bind_cols(test) %>% metrics(truth = count, estimate = .pred)

predict_train <- lm_fit %>% predict(test) %>% bind_cols(test) 

# predict_train$.pred %>% hist()

ggplot(data = predict_train, mapping = aes(x = .pred)) + 
  geom_histogram(binwidth = 25)

```


Task 4
The histrogram is somewhat bimodal with the center near 200.  The spread is between -200 and 600 with no gaps or extremes.  


Task 5
Peformance on the test set (rsq 0.6266689) is similar to the performance on the training set.  This most likely indicates the model is not overfitting. 

